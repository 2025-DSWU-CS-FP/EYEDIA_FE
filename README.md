# Eyedia <img src="https://eyedia.netlify.app/logo.svg" width="110" align="left" />
LLMê³¼ RAG ê¸°ìˆ ì„ í™œìš©í•œ AIê¸°ë°˜ ë¯¸ìˆ ê´€ ë„ìŠ¨íŠ¸ ì„œë¹„ìŠ¤

<br />

## **ğŸ’¡1. í”„ë¡œì íŠ¸ ê°œìš”**

### 1-1. í”„ë¡œì íŠ¸ ì†Œê°œ
- í”„ë¡œì íŠ¸ ëª… : LLMê³¼ RAG ê¸°ìˆ ì„ í™œìš©í•œ AIê¸°ë°˜ ë¯¸ìˆ ê´€ ë„ìŠ¨íŠ¸ ì„œë¹„ìŠ¤
- í”„ë¡œì íŠ¸ ì •ì˜ : ì‚¬ìš©ìê°€ ì‹œì„  ì¶”ì ì„ í†µí•´ ì›í•˜ëŠ” ë¶€ë¶„ë§Œ ì§ˆì˜í•˜ë©´, AIë¡œ í•´ë‹¹ ë¶€ë¶„ì— ëŒ€í•œ ì„¤ëª…ì„ ì°¾ì•„ì„œ ë‹µë³€í•´ì£¼ëŠ” ëŒ€í™”í˜• ë„ìŠ¨íŠ¸ ì„œë¹„ìŠ¤

<img width="1708" height="960" alt="image" src="https://eyedia.netlify.app/image.png" /></br>

<br />

### 1-2. ê°œë°œ ë°°ê²½ ë° í•„ìš”ì„±

ë³¸ í”„ë¡œì íŠ¸ëŠ” ìŠ¤ë§ˆíŠ¸ ì•„ì´ì›¨ì–´(Jetson Nano) , LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸), RAG(ë¬¸ì„œ ê²€ìƒ‰ ê¸°ë°˜ ìƒì„± ëª¨ë¸)ì„ í™œìš©í•˜ì—¬ AI ê¸°ë°˜ ë¯¸ìˆ ê´€ ë„ìŠ¨íŠ¸ ì‹œìŠ¤í…œì„ ê°œë°œí•©ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ì›¹ í™˜ê²½ì—ì„œ ë¯¸ìˆ ê´€ì˜ íŠ¹ì • ê·¸ë¦¼, íŠ¹ì • ê°ì²´ì— ëŒ€í•´ ì§ˆë¬¸í•˜ë©´, ê·¸ì— ëŒ€í•œ ìì—°ìŠ¤ëŸ½ê³  ìì„¸í•œ ì„¤ëª…ì„ ì œê³µí•©ë‹ˆë‹¤. </br></br>


<img width="3840" height="2160" alt="image" src="https://github.com/user-attachments/assets/878db6e0-ebba-4aa9-ae3c-e754b2bf8f15" />

<img width="3840" height="2160" alt="image" src="https://github.com/user-attachments/assets/4e3aa93c-8da1-4271-b25d-4918508165d6" />

<img width="5704" height="2172" alt="image" src="https://github.com/user-attachments/assets/8b7d6b68-bc32-42fe-a0a2-118e86f66d7f" />
<br />

<br />

### 1-3. í”„ë¡œì íŠ¸ íŠ¹ì¥ì 
- ì‹¤ì‹œê°„ ë§ì¶¤í˜• AI ë„ìŠ¨íŠ¸
- ì‹œì„ Â·ê´€ì‹¬ ê°ì²´Â·ìŒì„± ì§ˆì˜ ë“± ë‹¤ì–‘í•œ ì…ë ¥ì„ ìˆ˜ì§‘Â·í•´ì„í•´ ì¦‰ì„ ì„¤ëª… ìƒì„±
- ì±„íŒ…Â·ìŒì„±(TTS)ë¡œ ê²°ê³¼ ì œê³µ, ê´€ëŒ íë¦„ì— ë§ì¶˜ ì¸í„°ë™í‹°ë¸Œ ê²½í—˜ ì œê³µ
- ë¯¸ë¦¬ ë…¹ìŒëœ ì¼ë°©í–¥ ì„¤ëª…Â·GPS/QR/ë²„íŠ¼ ì…ë ¥ ì¤‘ì‹¬ì˜ ê¸°ì¡´ ë„ìŠ¨íŠ¸ ë°©ì‹ì—ì„œ íƒˆí”¼
- ì‹œì„  ì¶”ì Â·ê°ì²´ ì¸ì‹Â·ë©€í‹°ëª¨ë‹¬ AI(ì˜ìƒÂ·ìŒì„±Â·í…ìŠ¤íŠ¸ í†µí•©)ë¡œ ê´€ëŒê°ì˜ â€˜ê´€ì‹¬â€™ê³¼ â€˜í–‰ë™â€™ ì¤‘ì‹¬ ì„¤ê³„
- ìƒìš© ì•ˆë‚´ê¸°ì˜ íì‡„ì„±Â·ê³ ê°€ ëŒ€ë¹„, ì˜¤í”ˆì†ŒìŠ¤ ê¸°ë°˜ìœ¼ë¡œ ê¸°ëŠ¥ ììœ  ì¶”ê°€Â·ë³€ê²½Â·í™•ì¥ ê°€ëŠ¥
- ì‹œì„ (ë™ê³µ)Â·í™˜ê²½(ì‘í’ˆ)Â·ìŒì„±(ì§ˆì˜)Â·í”„ë¡ íŠ¸ í´ë¦­ ë“± ëª¨ë“  ì…ë ¥ì„ ì‹¤ì‹œê°„ í†µí•© ë¶„ì„
- ì§„ì •í•œ ë§ì¶¤í˜• ë„ìŠ¨íŠ¸ ê²½í—˜ êµ¬í˜„, í˜„ì¥ ì ìš© ê°€ëŠ¥ì„±ê³¼ í™•ì¥ì„± ì…ì¦

<br />


### 1-4. ì£¼ìš” ê¸°ëŠ¥
- **ê°ì²´ ì¸ì‹Â·í¬ë¡­ ìë™í™”**: Ultralytics YOLOv8ë¡œ ì‘í’ˆ ë‚´ ê°ì²´ bbox íƒì§€ â†’ OpenCVë¡œ ê°ì²´ë³„ crop ì €ì¥/ë¼ë²¨ ë§¤í•‘
- **ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰**: CLIP ì„ë² ë”© + FAISS ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ ê°€ì¥ ìœ ì‚¬í•œ ê°ì²´ì™€ ì„¤ëª…ì„ ìë™ ë°˜í™˜
- **LLM ë„ìŠ¨íŠ¸ ì„¤ëª… ìƒì„±**: GPT-4oì— í”„ë¡¬í”„íŠ¸ ì „ë‹¬ â†’ í•œêµ­ì–´ êµ¬ì–´ì²´ ì„¤ëª… ìƒì„± â†’ ì±„íŒ… í˜•ì‹ìœ¼ë¡œ ë°±ì—”ë“œ ì „ì†¡(FastAPI)
- **ì‹œì„  ì¶”ì Â·ê°ì²´ ì„ íƒ(Gaze)**: OpenCV/MediaPipeë¡œ ë™ê³µ ì¤‘ì‹¬ ì¶”ì¶œ â†’ Homography ê¸°ë°˜ Gaze Mapping â†’ YOLO bboxì™€ êµì°¨í•´ â€˜ì‘ì‹œ ê°ì²´â€™ íŒì • ë° ì‹œì„  ì‹œê°í™”
- **ì‹¤ì‹œê°„ ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸**: Jetson Nanoê°€ í™˜ê²½/ë™ê³µ ì˜ìƒ+ìŒì„± ìˆ˜ì§‘â†’ Wi-Fië¡œ FastAPI ì „ì†¡ â†’ YOLOÂ·CLIPÂ·FAISS ì²˜ë¦¬ â†’ STT/TTS â†’ Spring Boot ì €ì¥/ê¶Œí•œ â†’ React ì±„íŒ… UI
- **ìŠ¤ë§ˆíŠ¸ ì•„ì´ì›¨ì–´ H/W**: Jetson Nano + Camera Module 3(í™˜ê²½) + Camera Module 3(ëˆˆë™ì) + Wi-Fi
- **ì‹¤ì‹œê°„ í†µì‹ /ë°ì´í„° ì—°ë™**: Jetson Nano â†” ë°±ì—”ë“œ ê°„ ì‹¤ì‹œê°„ ì†¡ìˆ˜ì‹ (ì˜ìƒ/ì‹œì„ /ìŒì„±), ì—°ë™ ì™„ë£Œ
- **í´ë¼ìš°ë“œ ë°ì´í„° ê´€ë¦¬**: ì´ë¯¸ì§€/ë©”íƒ€ë°ì´í„°ëŠ” S3, ê´€ê³„ ë°ì´í„°ëŠ” RDS(MySQL)ì— ì•ˆì „ ì €ì¥
- **ì›¹ì•± ì£¼ìš” ê¸°ëŠ¥**: ë©”ì¸/ê°¤ëŸ¬ë¦¬ ê°ìƒ, ìŠ¤ë§ˆíŠ¸ ì•„ì´ì›¨ì–´ ì—°ë™ ì±„íŒ…, ëŒ€í™” ê¸°ë¡ í™•ì¸Â·ê²€ìƒ‰, ë°œì·Œ ê¸°ëŠ¥, TTS, STT


<br />

### 1-5. ê¸°ëŒ€ íš¨ê³¼ ë° í™œìš© ë¶„ì•¼

ê¸°ëŒ€ íš¨ê³¼
> ê´€ëŒê°ì˜ ì‹œì„ Â·ì§ˆì˜Â·í–‰ë™ì„ í† ëŒ€ë¡œ í•´ì„¤ì„ ì¦‰ì‹œ ìƒì„±í•´ ê¸°ì¡´ ì˜¤ë””ì˜¤ ê°€ì´ë“œë³´ë‹¤ ëª°ì…ê°ê³¼ ë§Œì¡±ë„ë¥¼ ë†’ì´ê³ , ì‹œì„ Â·ì§ˆë¬¸Â·ì²´ë¥˜ì‹œê°„ ë“± í–‰ë™ ë°ì´í„°ë¥¼ ì¶•ì Â·ë¶„ì„í•´ íë ˆì´ì…˜ê³¼ ë™ì„  ì„¤ê³„ë¥¼ ì§€ì†ì ìœ¼ë¡œ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸Â·ìŒì„±(TTS)ì„ ë™ì‹œ ì œê³µí•´ ì—°ë ¹Â·ì¥ì•  ìœ ë¬´ì™€ ìƒê´€ì—†ì´ ì ‘ê·¼ì„±ì„ ê°•í™”í•˜ê³ , ì›¹(React)Â·ë°±ì—”ë“œ(Spring) ê¸°ë°˜ êµ¬ì¡°ì™€ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë“ˆëŸ¬ ì„¤ê³„ë¡œ í˜„ì¥/ì¥ë¹„ ì œì•½ ì—†ì´ ë¹ ë¥¸ ë°°í¬ì™€ ê¸°ëŠ¥ í™•ì¥ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. íŒ€ê³¼ ê¸°ê´€ ì…ì¥ì—ì„œëŠ” YOLOÂ·CLIPÂ·LLMÂ·ì‹œì„ ì¶”ì  ë“± ìœµí•© ê¸°ìˆ ì„ ì‹¤ì „ì—ì„œ ë‹¤ë£¨ë©° ìš´ì˜ ìë™í™” íŒŒì´í”„ë¼ì¸ì„ ì¶•ì í•´ ì¸ë ¥Â·ë¹„ìš© íš¨ìœ¨ì„ ë™ì‹œì— í™•ë³´í•©ë‹ˆë‹¤.

í™œìš© ë¶„ì•¼
> ë¯¸ìˆ ê´€Â·ë°•ë¬¼ê´€Â·ê¸°ë…ê´€ ë“± ì „ì‹œ ê³µê°„ì—ì„œ ì‘í’ˆ ë§ì¶¤ ë„ìŠ¨íŠ¸ë¡œ ì¦‰ì‹œ ì ìš©í•  ìˆ˜ ìˆê³ , ê³¼í•™ê´€Â·ê¸°ì—… ì‡¼ë£¸Â·ë¸Œëœë“œ íŒì—…/ë¦¬í…Œì¼ ì „ì‹œì—ì„œëŠ” ì œí’ˆÂ·í”„ë¡œí† íƒ€ì… ì´í•´ë¥¼ ë•ëŠ” ì¸í„°ë™í‹°ë¸Œ ì„¤ëª…ìœ¼ë¡œ ì „í™˜ìœ¨ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë„ì‹œ ê´€ê´‘ì§€Â·ë¬¸í™”ì¬ í˜„ì¥Â·ì•¼ì™¸ í˜ìŠ¤í‹°ë²Œ/ë¹„ì—”ë‚ ë ˆì—ì„œëŠ” GPSÂ·ë¹„ì „ ê²°í•© ì•ˆë‚´ë¡œ í˜„ì¥ì„±ì„ ê°•í™”í•˜ê³ , í•™êµÂ·ëŒ€í•™Â·ë„ì„œê´€Â·ê³µê³µê¸°ê´€ì—ì„œëŠ” êµìœ¡ìš© ì²´í—˜í˜• ì½˜í…ì¸ ì™€ ì ‘ê·¼ì„± ë³´ì¡° í•´ì„¤ë¡œ í•™ìŠµ íš¨ê³¼ë¥¼ ë†’ì…ë‹ˆë‹¤. ê³µí•­Â·ì—­ì‚¬Â·ë°•ëŒíšŒ ê°™ì€ ëŒ€ê·œëª¨ ìœ ë™ ì¸í”„ë¼ì—ì„œëŠ” ë‹¤êµ­ì–´ ì•ˆë‚´ì™€ êµ°ì¤‘ í–‰íƒœ ë°ì´í„° ë¶„ì„ì—, ìŠ¤ë§ˆíŠ¸ ì•„ì´ì›¨ì–´ì™€ ê²°í•©í•œ íˆ¬ì–´Â·ì´ë²¤íŠ¸ ìš´ì˜ì—ëŠ” ê²½ëŸ‰ ì¥ë¹„ ê¸°ë°˜ì˜ ì´ë™í˜• ê°€ì´ë“œë¡œ í™•ì¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.

<br />

### 1-6. ê¸°ìˆ  ìŠ¤íƒ

<table>
  <thead>
    <tr>
      <th>êµ¬ë¶„</th>
      <th>í•­ëª©</th>
      <th>ìƒì„¸ë‚´ìš©</th>
    </tr>
  </thead>
  <tbody>
    <!-- S/W ê°œë°œí™˜ê²½ -->
    <tr>
      <td rowspan="5"><strong>S/W ê°œë°œí™˜ê²½</strong></td>
      <td>OS</td>
      <td>macOS, Ubuntu 22.04 (AWS EC2), Windows</td>
    </tr>
    <tr>
      <td>ê°œë°œí™˜ê²½(IDE)</td>
      <td>VSCode, IntelliJ, Colab</td>
    </tr>
    <tr>
      <td>ê°œë°œë„êµ¬</td>
      <td>Spring Boot, React, FastAPI, FAISS, Docker, GitHub Actions</td>
    </tr>
    <tr>
      <td>ê°œë°œì–¸ì–´</td>
      <td>Java, TypeScript (JSX/TSX), Python, MySQL</td>
    </tr>
    <tr>
      <td>ê¸°íƒ€ì‚¬í•­</td>
      <td>AWS EC2/S3/Route53, OpenAI(GPT-4, TTS)</td>
    </tr>
    <!-- H/W êµ¬ì„±ì¥ë¹„ -->
    <tr>
      <td rowspan="4"><strong>H/W êµ¬ì„±ì¥ë¹„</strong></td>
      <td>ë””ë°”ì´ìŠ¤</td>
      <td>Jetson Nano (Jetson Orin Nano Super) + ì¹´ë©”ë¼ 2ëŒ€ + usb ë¦¬ëª¨ì»¨</td>
    </tr>
    <tr>
      <td>í†µì‹ </td>
      <td>Wi-Fi, HTTP API í†µì‹  (FastAPI â†” Spring Boot â†” React)</td>
    </tr>
    <tr>
      <td>ì–¸ì–´</td>
      <td>Python ê¸°ë°˜ ì œì–´ìŠ¤í¬ë¦½íŠ¸, OpenCV</td>
    </tr>
    <tr>
      <td>ê¸°íƒ€ì‚¬í•­</td>
      <td>Dual camera frame capture, ì‹œì„  ì¢Œí‘œ ì¶”ì •(Gaze Estimation), ì‹¤ì‹œê°„ crop ê°ì²´ ì‹ë³„</td>
    </tr>
    <!-- í”„ë¡œì íŠ¸ ê´€ë¦¬í™˜ê²½ -->
    <tr>
      <td rowspan="3"><strong>í”„ë¡œì íŠ¸ ê´€ë¦¬í™˜ê²½</strong></td>
      <td>í˜•ìƒê´€ë¦¬</td>
      <td>Git, GitHub Actions CI/CD, Docker</td>
    </tr>
    <tr>
      <td>ì˜ì‚¬ì†Œí†µê´€ë¦¬</td>
      <td>Discord, ì¹´ì¹´ì˜¤í†¡, GitHub Issues</td>
    </tr>
    <tr>
      <td>ê¸°íƒ€ì‚¬í•­</td>
      <td>Notion, Google Drive, Google Sheets, Figma (UX ì‹œì•ˆ)</td>
    </tr>
  </tbody>
</table>


---

## **ğŸ’¡2. íŒ€ì› ì†Œê°œ**

| [<img src="https://github.com/Dubabbi.png" width="150px">](https://github.com/Dubabbi) | [<img src="https://github.com/kcw9609.png" width="150px">](https://github.com/kcw9609) | [<img src="https://github.com/20210699.png" width="150px">](https://github.com/20210699) | [<img src="https://github.com/yuchaemin2.png" width="150px">](https://github.com/yuchaemin2) | [<img src="https://github.com/yooni1231.png" width="150px">](https://github.com/yooni1231) |
|:---:|:---:|:---:|:---:|:---:|
| [ìœ¤ì†Œì€](https://github.com/Dubabbi)   | [ê°•ì±„ì›](https://github.com/kcw9609)   | [ê¹€ì˜ˆë¹ˆ](https://github.com/20210699) | [ìœ ì±„ë¯¼](https://github.com/yuchaemin2) | [ì´ìœ¤ì„œ](https://github.com/yooni1231) |
| â€¢ íŒ€ì¥ <br> â€¢ í”„ë¡ íŠ¸ì—”ë“œ | â€¢ ë¶€íŒ€ì¥, ë°±ì—”ë“œ ì´ê´„ <br> â€¢ ë°±ì—”ë“œ, ëª¨ë¸  | â€¢ ì„œê¸° <br> â€¢ CI/CD êµ¬ì¶•, ë°±ì—”ë“œ, ëª¨ë¸ |â€¢ ë¹„êµê³¼ ì´ê´„ <br> â€¢ ì‘ì—…/ë³´ê³ ì„œ ê´€ë¦¬ <br> â€¢ ë°±ì—”ë“œ, ëª¨ë¸| â€¢ llm & ë°ì´í„°ë¶„ì„ íŒŒíŠ¸ì¥ <br> â€¢ í•˜ë“œì›¨ì–´, ëª¨ë¸ |

## ğŸ–¼ï¸ ë‹¨ì²´ ì‚¬ì§„
> ìš°ë¦¬ íŒ€ ìµœê³ 

![KakaoTalk_20250926_103746930](https://github.com/user-attachments/assets/5758f16b-64f5-44ff-b23d-e73ed3bb602d)

---
## **ğŸ’¡3. ì‹œìŠ¤í…œ êµ¬ì„±ë„**


| ì„œë¹„ìŠ¤ êµ¬ì„±ë„ | ì‹œìŠ¤í…œ êµ¬ì„±ë„ |
|---------------|---------------|
| <img width="800" height="530" alt="á„‰á…µá„‚á…¡á„…á…µá„‹á…©-2" src="https://github.com/user-attachments/assets/ebf44098-51b0-475d-baa6-13ff57a134b6" />| <img width="320" alt="image" src="https://github.com/user-attachments/assets/24654a54-4c53-4b31-8604-854386241509" /> | 


|ì—”í‹°í‹° ê´€ê³„ë„ |
|---------------|
| <img alt="image" src="https://github.com/user-attachments/assets/d7ae2534-8479-4a58-8778-6e110f80b84a" />|
|ì•„ì´ì›¨ì–´ ì—°ê²° â†’ ì‹¤ì‹œê°„ ì±„íŒ… flow |
| <img alt="ì‹¤ì‹œê°„ ì±„íŒ… flow" src="https://github.com/user-attachments/assets/866d1f77-218b-4d0e-87de-4d3f139cc9fb" /> |
|ì†Œí”„íŠ¸ì›¨ì–´ ì•„í‚¤í…ì²˜ | 
| <img alt="ì†Œí”„íŠ¸ì›¨ì–´ ì•„í‚¤í…ì²˜" src="https://github.com/user-attachments/assets/fd473c00-e4a8-43a2-a77a-700d3a756f56" />|
|í•˜ë“œì›¨ì–´ ì„¤ê³„ |
| <img alt="image" src="https://github.com/user-attachments/assets/0949f869-6451-42fb-b65e-b94d873ded62" />|

<br />

## **ğŸ’¡4. ì‘í’ˆ ì†Œê°œì˜ìƒ**
### ğŸ”—[Eyedia í”„ë¡œì íŠ¸ ì†Œê°œ ì˜ìƒ ë³´ëŸ¬ê°€ê¸°](https://www.youtube.com/watch?v=akFH7fvptO8)
### ğŸ”—[Eyedia íŒ€ì˜ ì›ƒí”ˆ í”„ë¡œì íŠ¸ ê³¼ì • ë³´ëŸ¬ê°€ê¸°](https://youtube.com/shorts/cawQaq1wMBc?si=ymtyX2Hwrhz8wHAg)

<br />

## **ğŸ’¡5. í•µì‹¬ ì†ŒìŠ¤ì½”ë“œ**

<details>
  <summary><h3>ë°±ì—”ë“œ ë„ìŠ¨íŠ¸ í”„ë¡¬í”„íŠ¸</h3></summary>

```Java
public Prompt basePrompt(Painting p, String question){

        String system = """
            ë„ˆëŠ” ë¯¸ìˆ ê´€ ë„ìŠ¨íŠ¸ì•¼. í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•´.
            - ì œê³µëœ ì‘í’ˆ ì •ë³´ì™€ ì§ˆë¬¸ë§Œ ì‚¬ìš©(ì¶”ì¸¡ ê¸ˆì§€)
            - 3~6ë¬¸ì¥, ë§ˆì§€ë§‰ì— ì§§ì€ ì§ˆë¬¸ìœ¼ë¡œ ë§ˆë¬´ë¦¬
            """;

        String user = """
            [ì‘í’ˆ ì •ë³´]
            ì œëª©: %s
            ì‘ê°€: %s
            ì „ì‹œ: %s
            ê¸°ë³¸ì„¤ëª…: %s

            [ê´€ëŒê° ì§ˆë¬¸]
            %s
            """.formatted(
                nz(p.getTitle()), nz(p.getArtist()),
                "ë©”íŠ¸ë¡œí´ë¦¬íƒ„ ë¯¸ìˆ ê´€", nz(p.getDescription()),
                nz(question)
        );

        return new Prompt(system, user);
    }

    public Prompt gazeAreaPrompt(String paintingTitle, String quadrant, String description){
        String system = """
                ë‹¹ì‹ ì€ ë¯¸ìˆ ê´€ì˜ ë„ìŠ¨íŠ¸ì…ë‹ˆë‹¤.\s
                ê´€ëŒê°ì—ê²Œ ì¹œì ˆí•˜ê³  ê°ì„±ì ì¸ ì–´ì¡°ë¡œ ì‘í’ˆ ì† ê°ì²´ë¥¼ ì„¤ëª…í•´ì•¼ í•©ë‹ˆë‹¤.\s
                ë„ˆë¬´ ê¸°ìˆ ì ì´ê±°ë‚˜ ë”±ë”±í•˜ì§€ ì•Šê²Œ í’€ì–´ì£¼ì„¸ìš”.\s
                ê·¸ë¦¼ì— ì—†ëŠ” ë‚´ìš©ì€ ì„¤ëª…í•˜ì§€ ë§ê³ , í•´ë‹¹ ê·¸ë¦¼ ì™¸ì˜ ë‹¤ë¥¸ ê·¸ë¦¼ì€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.\s
                ì‘í’ˆ ì „ì²´ ì„¤ëª…ë³´ë‹¤ëŠ” ê° ê°ì²´ì— ëŒ€í•œ ì„¤ëª…ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•´ì„¤í•´ì£¼ì„¸ìš”.
            """;

        String user = """
                ì•„ë˜ëŠ” %s ê·¸ë¦¼ì˜ %s ë¶„ë©´ì—ì„œ ê°ì§€ëœ í›„ë³´ ê°ì²´ ì„¤ëª…ì…ë‹ˆë‹¤: \s
                %s\s
                â†’ ìœ„ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê´€ëŒê°ì—ê²Œ ì„¤ëª…ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
            """.formatted(
                nz(paintingTitle), nz(quadrant),
                nz(description)
        );

        return new Prompt(system, user);
    }
```
</details>
<details>
  <summary><h3>ë°±ì—”ë“œ ê·¸ë¦¼ ì¸ì‹ ì½”ë“œ</h3></summary>

```Java
public ApiResponse<?> detect(@RequestBody Long artId) {

        MessageDTO.ChatImageResponseDTO message;

        var list = paintingRepository.findNullUserByArtId(artId);
        if (list.isEmpty()) {
            throw new GeneralException(ErrorStatus.PAINTING_NOT_FOUND);
        } else if (list.size() > 1) {
            List<Long> ids = list.stream().map(Painting::getPaintingId).toList();
            throw new GeneralException(ErrorStatus.PAINTING_CONFLICT, Map.of("duplicatedPaintingIds", ids));
        } else {
            Painting painting = list.get(0);
            var exhibition = exhibitionRepository.findByPaintingsPaintingId(painting.getPaintingId())
                    .orElseThrow(() -> new GeneralException(ErrorStatus.EXHIBITION_NOT_FOUND));

            message = MessageDTO.ChatImageResponseDTO.builder()
                    .paintingId(painting.getPaintingId())
                    .imgUrl("https://s3-eyedia.s3.ap-northeast-2.amazonaws.com/"
                            + exhibition.getExhibitionsId() + "/" + artId + "/" + artId + ".jpg")
                    .title(painting.getTitle())
                    .artist(painting.getArtist())
                    .description(painting.getDescription())
                    .exhibition(exhibition.getTitle())
                    .artId(artId)
                    .build();

            messagingTemplate.convertAndSend("/queue/events", message);
        }
        return ApiResponse.of(SuccessStatus._OK, message);
    }
```
</details>

<details>
  <summary><h3>í”„ë¡ íŠ¸ì—”ë“œ STOMP WebSocket í›… (ì—°ê²°Â·êµ¬ë…Â·ì†¡ì‹ )</h3></summary>

```tsx
import { useCallback, useEffect, useMemo, useRef, useState } from 'react';
import { IMessage, StompSubscription, IFrame } from '@stomp/stompjs';
import makeStompClient from '@/services/ws/makeStompClient';
import type { IncomingChat, OutgoingChat } from '@/types/chat';

type Args = {
  paintingId: number;
  token?: string;
  onConnected?: (headers: IFrame['headers']) => void;
  topic?: string;
  onRoomMessage?: (msg: IMessage) => void;
};

export default function useStompChat({
  paintingId, token, onConnected, topic: topicOverride, onRoomMessage,
}: Args) {
  const [connected, setConnected] = useState(false);
  const [messages, setMessages] = useState<IncomingChat[]>([]);
  const clientRef = useRef<ReturnType<typeof makeStompClient> | null>(null);
  const chatSubRef = useRef<StompSubscription | null>(null);
  const roomSubRef = useRef<StompSubscription | null>(null);

  const chatTopic = useMemo(
    () => topicOverride ?? `/topic/chat/art/${paintingId}`,
    [topicOverride, paintingId],
  );
  const roomTopic = useMemo(() => `/room/${paintingId}`, [paintingId]);

  useEffect(() => {
    const url = import.meta.env.VITE_WS_URL as string;
    const client = makeStompClient({ url, token });
    clientRef.current = client;

    client.onConnect = (frame) => {
      setConnected(true);
      onConnected?.(frame.headers);

      chatSubRef.current = client.subscribe(chatTopic, (f) => {
        try { setMessages((p) => [...p, JSON.parse(f.body)]); } catch {}
      });
      roomSubRef.current = client.subscribe(roomTopic, (f) => onRoomMessage?.(f));
    };

    client.onWebSocketClose = () => setConnected(false);
    client.activate();

    return () => {
      chatSubRef.current?.unsubscribe();
      roomSubRef.current?.unsubscribe();
      client.deactivate();
    };
  }, [token, chatTopic, roomTopic]);

  const send = useCallback((content: string) => {
    const body = JSON.stringify({ paintingId, content: content.trim() } as OutgoingChat);
    if (content.trim()) clientRef.current?.publish({ destination: '/app/chat.sendMessage', body });
  }, [paintingId]);

  return { connected, messages, send, disconnect: () => clientRef.current?.deactivate() };
}
```

</details>


<details>
  <summary><h3>í”„ë¡ íŠ¸ì—”ë“œ ì•„ì´ì›¨ì–´ ì—°ê²° ê°ì§€</h3></summary>

```tsx
import { useEffect, useMemo, useState } from 'react';
import { useNavigate } from 'react-router-dom';
import useStompChat from '@/hooks/use-stomp-chat';
import getAuthToken from '@/utils/getToken';

type QueueEvent = {
  paintingId?: number; imgUrl?: string; title?: string; artist?: string;
  description?: string; exhibition?: string; artId?: number;
};

export default function OnboardingPage() {
  const [isConnected, setIsConnected] = useState(false);
  const [userId, setUserId] = useState<string>();
  const [detected, setDetected] = useState<QueueEvent | null>(null);
  const navigate = useNavigate();
  const token = getAuthToken();

  const { connected, messages } = useStompChat({
    paintingId: 0, token, topic: '/queue/events',
    onConnected: (headers) => {
      setIsConnected(true);
      const uid = headers['user-name'] as string | undefined;
      if (uid) setUserId(uid);
    },
  });

  useEffect(() => {
    const items = Array.isArray(messages) ? (messages as QueueEvent[]) : [];
    const next = items.find((e) => typeof e?.paintingId === 'number');
    if (next) setDetected(next);
  }, [messages]);

  useEffect(() => {
    if (!detected?.paintingId) return;
    const t = window.setTimeout(() => {
      navigate('/chat-gaze', {
        state: { userId, ...detected, artId: detected.artId ?? detected.paintingId },
      });
    }, 3000);
    return () => window.clearTimeout(t);
  }, [detected, userId, navigate]);

  const text = useMemo(
    () => (connected || isConnected) ? 'ì•„ì´ì›¨ì–´ ì—°ê²° ì„±ê³µ' : 'ì „ìš© ì•„ì´ì›¨ì–´ë¥¼ ì—°ê²°í•´ì£¼ì„¸ìš”.',
    [connected, isConnected],
  );

  return (
    // ì—°ê²° ìƒíƒœì— ë”°ë¼ ì˜¨ë³´ë”© UI & ì—°ê²° ì• ë‹ˆë©”ì´ì…˜ ë Œë”
    // ...
    <div aria-live="polite">{text}</div>
  );
}
```

</details>


<details>
  <summary><h3>í”„ë¡ íŠ¸ì—”ë“œ ì‘í’ˆ í™•ì • í›„ ëŒ€í™” ì‹œì‘</h3></summary>

```tsx
import { useCallback, useEffect, useMemo, useState } from 'react';
import { useLocation, useNavigate } from 'react-router-dom';
import useStompChat from '@/hooks/use-stomp-chat';
import useConfirmPainting from '@/services/mutations/useConfirmPainting';

type LocationState = {
  paintingId?: number; imgUrl?: string; title?: string; artist?: string;
  description?: string; exhibition?: string; artId?: number;
};

export default function GazePage() {
  const { state } = useLocation();
  const s = (state as LocationState | null) ?? null;
  const navigate = useNavigate();
  const { mutate: confirmPainting, isPending } = useConfirmPainting();

  const hasDetected = typeof s?.paintingId === 'number';
  const pid = useMemo(() => (hasDetected ? (s!.paintingId as number) : -1), [hasDetected, s?.paintingId]);

  const [minSpinDone, setMinSpinDone] = useState(false);
  useEffect(() => { const t = setTimeout(() => setMinSpinDone(true), 2000); return () => clearTimeout(t); }, []);
  const ready = hasDetected && minSpinDone;

  useStompChat({ paintingId: pid });

  const handleStartConversation = useCallback(() => {
    if (pid < 0) return;
    confirmPainting(pid, {
      onSettled: () => {
        navigate('/chat-artwork', {
          state: { paintingId: pid, title: s?.title, artist: s?.artist, imgUrl: s?.imgUrl,
                   description: s?.description, exhibition: s?.exhibition },
          replace: true,
        });
      },
    });
  }, [confirmPainting, navigate, pid, s]);

  return (
    <button disabled={!ready || isPending} onClick={handleStartConversation}>ëŒ€í™” ì‹œì‘í•˜ê¸°</button>
  );
}
```

</details>


<details>
  <summary><h3>í”„ë¡ íŠ¸ì—”ë“œ Artwork ëŒ€í™”</h3></summary>

```tsx
import { useMemo, useRef, useEffect, useState, useCallback } from 'react';
import { nanoid } from 'nanoid';
import { useNavigate, useLocation } from 'react-router-dom';
import useStompChat from '@/hooks/use-stomp-chat';
import useArtworkChat from '@/hooks/useArtworkChat';
import useAutoScrollToEnd from '@/hooks/useAutoScrollToEnd';
import useRoomMessageHandler from '@/hooks/useRoomMessageHandler';
import useChatMessages from '@/services/queries/useChatMessages';
import getAuthToken from '@/utils/getToken';
import type { IncomingChat } from '@/types/chat';

export default function ArtworkPage() {
  const { state } = useLocation() as { state: { paintingId: number; imgUrl?: string; title?: string; artist?: string } };
  const { paintingId, imgUrl, title = 'ì‘í’ˆ', artist = '' } = state;

  // ì´ˆê¸° ì„œë²„ ì €ì¥ ë©”ì‹œì§€ ë¶ˆëŸ¬ì˜¤ê¸°
  const { data: chatMessages } = useChatMessages(paintingId);
  const initial = useMemo(() => (chatMessages ?? []).map(m => ({
    id: nanoid(), content: m.content, sender: m.sender as 'USER' | 'BOT', type: 'TEXT' as const,
  })), [chatMessages]);

  // ë¡œì»¬/íƒ€ì/ë³´ì´ìŠ¤ ë“± ë³µí•© ì±„íŒ… í›…
  const {
    localMessages, setLocalMessages, typing, submitAsk, startVoice,
    promptText, voiceDisabled, didAutoAskRef, startTypewriter, speak,
  } = useArtworkChat({ paintingId, onError: () => {/* toast */} });

  // ë°© ì´ë²¤íŠ¸ ì²˜ë¦¬ í•¸ë“¤ëŸ¬(ì´ë¯¸ì§€ ë¶„ì„ ë“±)
  const onRoomMessage = useRoomMessageHandler({
    paintingId, artworkInfo: { imgUrl }, processedIdsRef: useRef(new Set<string>()),
    setLocalMessages, startTypewriter, speak,
  });

  // STOMP ì—°ê²° + ë£¸ í† í”½ êµ¬ë…
  const token = getAuthToken();
  const { connected, messages: wsMessages } = useStompChat({
    paintingId, token, topic: `/topic/llm.${paintingId}`, onRoomMessage,
  });

  const wsList = wsMessages.map((m: IncomingChat) => ({
    id: m.id ?? nanoid(), sender: m.sender, content: m.content, type: 'TEXT' as const,
  }));

  const endRef = useRef<HTMLDivElement>(null);
  useAutoScrollToEnd([chatMessages, wsMessages, localMessages, typing], endRef);

  return (
    <>
      {/* MessageListì— initial/ws/local/typing ì „ë‹¬ */}
      {/* footer: ìŒì„± startVoice ë²„íŠ¼, í‚¤ë³´ë“œ ì…ë ¥ í† ê¸€/ChatInputBar */}
      {/* promptText: ì—°ê²° ìƒíƒœ/ê°€ì´ë“œ ë¬¸êµ¬ í‘œì‹œ */}
    </>
  );
}
```

</details>

<details>
  <summary><h3>í”„ë¡ íŠ¸ì—”ë“œ ë°œì·Œ ì €ì¥(ìŠ¤í¬ë©)</h3></summary>

```tsx
import { useQueryClient } from '@tanstack/react-query';
import useSaveScrap from '@/services/mutations/useSaveScrap';
import dateKST from '@/utils/dateKST';

const queryClient = useQueryClient();
const { mutate: saveScrap, isPending: saving } = useSaveScrap();

function handleSaveExcerpt(quote: string, paintingId: number, exhibition: string, artist: string) {
  const excerpt = quote.trim();
  if (!excerpt) return /* toast: ì—†ìŒ */;

  saveScrap(
    { paintingId, date: dateKST(), excerpt, location: exhibition, artist },
    {
      onSuccess: () => {
        // í† ìŠ¤íŠ¸ í›„ ìµœì‹  ë¦¬ìŠ¤íŠ¸ ì¬ì¡°íšŒ
        queryClient.invalidateQueries({ queryKey: ['recentViewedArtworks'] });
        queryClient.invalidateQueries({ queryKey: ['scrapsByExhibition'] });
      },
      onError: () => { /* toast: ì‹¤íŒ¨ */ },
    },
  );
}
```

</details>

<details>
  <summary><h3>ëª¨ë¸ ë™ê³µ</h3></summary>

```Python
import os
import time
import cv2
import dlib
import joblib
import numpy as np
from typing import Dict, Tuple, Optional, List
from sklearn.ensemble import RandomForestClassifier

# ==============================
# 0) ì „ì—­ ì„¤ì •
# ==============================
MODE = os.getenv("MODE", "predict")  # 'collect' or 'predict'
PREDICTOR_PATH = os.getenv("DLIB_PREDICTOR", "shape_predictor_68_face_landmarks.dat")

# í™”ë©´ 4ë¶„í• (2x2)
SCREEN_ZONES: Dict[int, str] = {
    1: "Top-Left",
    2: "Top-Right",
    3: "Bot-Left",
    4: "Bot-Right"
}
SCREEN_WIDTH, SCREEN_HEIGHT = 1280, 720   # UI ë””ìŠ¤í”Œë ˆì´ ìº”ë²„ìŠ¤ í¬ê¸°
SAMPLES_PER_ZONE = 20                      # ê° êµ¬ì—­ ìˆ˜ì§‘ ìƒ˜í”Œ ìˆ˜
MODEL_PATH = "gaze_model.pkl"              # í•™ìŠµ ëª¨ë¸ ê²½ë¡œ

# dlib êµ¬ì„±
detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor(PREDICTOR_PATH)

# ==============================
# 1) íŠ¹ì§• ì¶”ì¶œ ìœ í‹¸ í•¨ìˆ˜
# ==============================
def get_eye_keypoints(
    shape: dlib.full_object_detection,
    gray_frame: np.ndarray,
    eye_points_indices: List[int]
) -> Tuple[Optional[Tuple[int, int]], Optional[Tuple[int, int]], Optional[Tuple[int, int]], Optional[Tuple[int, int]]]:
    """
    ì–¼êµ´ íŠ¹ì§•ì ì—ì„œ ëˆˆ ì˜ì—­ì„ ì˜ë¼ CLAHE+AdaptiveThresholdë¡œ ì´ì§„í™” í›„,
    ë™ê³µ(pupil)ê³¼ ê¸€ë¦°íŠ¸(glint) ì¢Œí‘œë¥¼ êµ¬í•œë‹¤.
    ë°˜í™˜: (inner_corner, outer_corner, pupil_center, glint_center)
           - ì¢Œí‘œëŠ” (x, y) ë˜ëŠ” None
    """
    eye_points = np.array([(shape.part(i).x, shape.part(i).y) for i in eye_points_indices], dtype=np.int32)
    x, y, w, h = cv2.boundingRect(eye_points)
    if w == 0 or h == 0:
        return None, None, None, None

    eye_roi = gray_frame[y:y + h, x:x + w]

    # ì•ˆìª½/ë°”ê¹¥ìª½ ëˆˆê¼¬ë¦¬ (indexëŠ” 68-landmark ê¸°ì¤€)
    inner_corner = (shape.part(eye_points_indices[3]).x, shape.part(eye_points_indices[3]).y)
    outer_corner = (shape.part(eye_points_indices[0]).x, shape.part(eye_points_indices[0]).y)

    # ëŒ€ë¹„ í–¥ìƒ
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    eye_roi = clahe.apply(eye_roi)

    # ì´ì§„í™” (ë™ê³µ í›„ë³´ ì¶”ì¶œ)
    threshold_eye = cv2.adaptiveThreshold(
        eye_roi, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
        cv2.THRESH_BINARY_INV, 11, 2
    )
    contours, _ = cv2.findContours(threshold_eye, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)

    # ì›í˜•ì„±(circularity)ê³¼ ë©´ì  ì œí•œìœ¼ë¡œ ë™ê³µ í›„ë³´ ì„ íƒ
    pupil_contour = None
    max_circularity = 0.0
    for c in contours:
        area = cv2.contourArea(c)
        if area == 0:
            continue
        perimeter = cv2.arcLength(c, True)
        if perimeter == 0:
            continue
        circularity = 4 * np.pi * (area / (perimeter * perimeter))
        # ê²½í—˜ì  ë²”ìœ„ (íŠœë‹ í¬ì¸íŠ¸): ì›í˜•ì„± 0.7~1.2, ë©´ì  15~400
        if 0.7 < circularity < 1.2 and 15 < area < 400:
            if circularity > max_circularity:
                max_circularity = circularity
                pupil_contour = c

    pupil_center: Optional[Tuple[int, int]] = None
    if pupil_contour is not None:
        M = cv2.moments(pupil_contour)
        if M['m00'] != 0:
            cx = int(M['m10'] / M['m00']) + x
            cy = int(M['m01'] / M['m00']) + y
            pupil_center = (cx, cy)

    # ê¸€ë¦°íŠ¸(í•˜ì´ë¼ì´íŠ¸)ëŠ” eye_roiì—ì„œ ê°€ì¥ ë°ì€ ì§€ì  (ê°„ë‹¨ ì¶”ì •)
    glint_center: Optional[Tuple[int, int]] = None
    if eye_roi.size > 0:
        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(eye_roi)
        # max_val ì„ê³„ì¹˜(ì˜ˆ: > 180)ëŠ” ì¡°ëª…/ë…¸ì´ì¦ˆì— ë”°ë¼ ì¡°ì • í•„ìš”
        if max_val > 180:
            glint_center = (max_loc[0] + x, max_loc[1] + y)

    return inner_corner, outer_corner, pupil_center, glint_center


def calculate_features(
    left_eye: Tuple[Optional[Tuple[int, int]], Optional[Tuple[int, int]], Optional[Tuple[int, int]], Optional[Tuple[int, int]]],
    right_eye: Tuple[Optional[Tuple[int, int]], Optional[Tuple[int, int]], Optional[Tuple[int, int]], Optional[Tuple[int, int]]]
) -> Optional[np.ndarray]:
    """
    ë…¼ë¬¸/ê²½í—˜ ê¸°ë°˜ì˜ 6ê³„ì—´ íŠ¹ì§•(ë²¡í„°+ê±°ë¦¬/ê°ë„)ì„ ì¡°í•©í•˜ì—¬ í•˜ë‚˜ì˜ feature vectorë¡œ ë§Œë“ ë‹¤.
    ì…ë ¥: ê° ëˆˆì— ëŒ€í•´ (inner_corner, outer_corner, pupil, glint)
    ì¶œë ¥: feature_vector (np.ndarray) ë˜ëŠ” None
    """
    if not all(p is not None for eye in [left_eye, right_eye] for p in eye):
        return None

    l_inner, _, l_pupil, l_glint = left_eye
    r_inner, _, r_pupil, r_glint = right_eye

    l_pupil, l_glint, l_inner = np.array(l_pupil), np.array(l_glint), np.array(l_inner)
    r_pupil, r_glint, r_inner = np.array(r_pupil), np.array(r_glint), np.array(r_inner)

    # ë²¡í„° êµ¬ì„±
    vec_l_pg = l_pupil - l_glint
    vec_r_pg = r_pupil - r_glint
    vec_l_pc = l_pupil - l_inner
    vec_r_pc = r_pupil - r_inner
    vec_l_gc = l_glint - l_inner
    vec_r_gc = r_glint - r_inner
    vec_cc = l_inner - r_inner

    # ê±°ë¦¬/ê°ë„
    dist_cc = np.linalg.norm(vec_cc)
    cos_theta_l = np.dot(vec_l_pg, vec_l_gc) / (np.linalg.norm(vec_l_pg) * np.linalg.norm(vec_l_gc) + 1e-6)
    theta_l = np.arccos(np.clip(cos_theta_l, -1.0, 1.0))
    cos_theta_r = np.dot(vec_r_pg, vec_r_gc) / (np.linalg.norm(vec_r_pg) * np.linalg.norm(vec_r_gc) + 1e-6)
    theta_r = np.arccos(np.clip(cos_theta_r, -1.0, 1.0))
    diff_cc = np.arctan2(vec_cc[1], vec_cc[0])

    feature_vector = np.concatenate([
        vec_l_pg, [np.linalg.norm(vec_l_pg)],
        vec_r_pg, [np.linalg.norm(vec_r_pg)],
        vec_l_pc, [np.linalg.norm(vec_l_pc)],
        vec_r_pc, [np.linalg.norm(vec_r_pc)],
        vec_l_gc, [np.linalg.norm(vec_l_gc)],
        vec_r_gc, [np.linalg.norm(vec_r_gc)],
        [dist_cc, theta_l, theta_r, diff_cc]
    ])
    return feature_vector


# ==============================
# 2) UI/ë„ì›€ í•¨ìˆ˜
# ==============================
def draw_screen_zones(canvas: np.ndarray) -> np.ndarray:
    """
    2x2 ê·¸ë¦¬ë“œë¥¼ ê·¸ë¦¬ê³ , ê° êµ¬ì—­ ë²ˆí˜¸(1~4)ë¥¼ í‘œì‹œí•œë‹¤.
    canvas í¬ê¸°ëŠ” SCREEN_WIDTH/HEIGHT ê¸°ë°˜.
    """
    rows, cols = 2, 2
    zone_w, zone_h = SCREEN_WIDTH // cols, SCREEN_HEIGHT // rows

    for i in range(1, rows * cols + 1):
        c = (i - 1) % cols
        r = (i - 1) // cols
        x1, y1 = c * zone_w, r * zone_h
        x2, y2 = x1 + zone_w, y1 + zone_h
        cv2.rectangle(canvas, (x1, y1), (x2, y2), (0, 255, 0), 2)
        cv2.putText(canvas, str(i), (x1 + 10, y1 + 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
    return canvas


def process_frame_get_features(frame: np.ndarray) -> Tuple[np.ndarray, Optional[np.ndarray]]:
    """
    í”„ë ˆì„ì—ì„œ ì–¼êµ´/ëˆˆ ëœë“œë§ˆí¬ë¥¼ ê²€ì¶œí•˜ê³ , ì–‘ ëˆˆì˜ íŠ¹ì§• ë²¡í„°ë¥¼ ì‚°ì¶œí•œë‹¤.
    - ì‹œê°í™”: ëœë“œë§ˆí¬(36~47), ë™ê³µ/ê¸€ë¦°íŠ¸ ì ì„ ì›ìœ¼ë¡œ í‘œì‹œ
    - ë°˜í™˜: (ì‹œê°í™”ëœ frame, features or None)
    """
    frame = cv2.flip(frame, 1)  # ì¢Œìš° ë°˜ì „(ìì—°ìŠ¤ëŸ¬ìš´ ì…€í”¼ ë°©í–¥)
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    faces = detector(gray)
    features: Optional[np.ndarray] = None

    for face in faces[:1]:  # ë‹¤ì¸ ê²€ì¶œ ì‹œ ì²« ì–¼êµ´ë§Œ ì‚¬ìš©(í•„ìš” ì‹œ ë” ê°œì„ )
        landmarks = predictor(gray, face)

        left_eye_indices = list(range(36, 42))
        right_eye_indices = list(range(42, 48))

        left_eye_keypoints = get_eye_keypoints(landmarks, gray, left_eye_indices)
        right_eye_keypoints = get_eye_keypoints(landmarks, gray, right_eye_indices)

        # ë””ë²„ê·¸ í‘œì‹œ: ëˆˆ ëœë“œë§ˆí¬ + ë™ê³µ(ë¹¨ê°•)/ê¸€ë¦°íŠ¸(íŒŒë‘)
        for i in range(36, 48):
            lx, ly = landmarks.part(i).x, landmarks.part(i).y
            cv2.circle(frame, (lx, ly), 2, (0, 255, 0), -1)
        if left_eye_keypoints[2]:
            cv2.circle(frame, left_eye_keypoints[2], 3, (0, 0, 255), -1)
        if left_eye_keypoints[3]:
            cv2.circle(frame, left_eye_keypoints[3], 3, (255, 0, 0), -1)
        if right_eye_keypoints[2]:
            cv2.circle(frame, right_eye_keypoints[2], 3, (0, 0, 255), -1)
        if right_eye_keypoints[3]:
            cv2.circle(frame, right_eye_keypoints[3], 3, (255, 0, 0), -1)

        # íŠ¹ì§• ë²¡í„° ê³„ì‚°
        features = calculate_features(left_eye_keypoints, right_eye_keypoints)

    return frame, features


def ensure_display_canvas() -> np.ndarray:
    """
    ì˜ˆì¸¡/ìˆ˜ì§‘ ì•ˆë‚´ UIë¥¼ ê·¸ë¦´ í‘ìƒ‰ ìº”ë²„ìŠ¤ë¥¼ ìƒì„±í•˜ê³ , 4êµ¬ì—­ ë¼ì¸ì„ ê·¸ë¦°ë‹¤.
    """
    display_frame = np.zeros((SCREEN_HEIGHT, SCREEN_WIDTH, 3), dtype=np.uint8)
    return draw_screen_zones(display_frame)


# ==============================
# 3) ë°ì´í„° ìˆ˜ì§‘/í•™ìŠµ/ì˜ˆì¸¡ ë£¨í”„
# ==============================
def collect_loop(cap: cv2.VideoCapture):
    """
    [COLLECT] ìŠ¤í˜ì´ìŠ¤ë°”ë¡œ í˜„ì¬ ì‘ì‹œ êµ¬ì—­ì˜ ìƒ˜í”Œì„ ìˆ˜ì§‘.
    - ê° êµ¬ì—­ SAMPLES_PER_ZONE ë§Œí¼ ëª¨ì´ë©´ ìë™ìœ¼ë¡œ ë‹¤ìŒ êµ¬ì—­
    - 's'ë¥¼ ëˆ„ë¥´ë©´ í•™ìŠµ+ì €ì¥ (ë‹¨, ëª¨ë“  êµ¬ì—­ ìˆ˜ì§‘ì´ ì™„ë£Œë˜ì–´ì•¼ í•¨)
    """
    features_data: List[np.ndarray] = []
    labels_data: List[int] = []

    current_zone = 1
    collected_counts = {i: 0 for i in range(1, 5)}

    print("--- ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“œ ---")
    print(f"ê° êµ¬ì—­ì„ ì‘ì‹œí•œ ìƒíƒœì—ì„œ 'ìŠ¤í˜ì´ìŠ¤ë°”'ë¡œ ë°ì´í„°ë¥¼ {SAMPLES_PER_ZONE}ê°œì”© ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    print("êµ¬ì—­ë³„ ìˆ˜ì§‘ì´ ì™„ë£Œë˜ë©´ ìë™ìœ¼ë¡œ ë‹¤ìŒ êµ¬ì—­ìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")

    while True:
        ok, frame = cap.read()
        if not ok:
            break

        vis_frame, features = process_frame_get_features(frame)
        display_frame = ensure_display_canvas()

        if current_zone <= 4:
            count = collected_counts[current_zone]
            text = f"Look at Zone [{current_zone}]  ({count}/{SAMPLES_PER_ZONE})  Press SPACE"
        else:
            text = "Collection Complete! Press 's' to train & save."
        cv2.putText(display_frame, text, (50, 50),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)

        cv2.imshow("Webcam Feed", vis_frame)
        cv2.imshow("Gaze Interface", display_frame)

        key = cv2.waitKey(1) & 0xFF
        if key == ord('q'):
            break

        # ìŠ¤í˜ì´ìŠ¤: í˜„ì¬ êµ¬ì—­ ìƒ˜í”Œ ìˆ˜ì§‘
        if key == ord(' ') and features is not None and current_zone <= 4:
            if collected_counts[current_zone] < SAMPLES_PER_ZONE:
                features_data.append(features)
                labels_data.append(current_zone)
                collected_counts[current_zone] += 1
                print(f"[Zone {current_zone}] {collected_counts[current_zone]}/{SAMPLES_PER_ZONE}")

            # í˜„ êµ¬ì—­ ìˆ˜ì§‘ ì™„ë£Œ â†’ ë‹¤ìŒ êµ¬ì—­ìœ¼ë¡œ
            if collected_counts[current_zone] == SAMPLES_PER_ZONE:
                print(f"Zone {current_zone} ìˆ˜ì§‘ ì™„ë£Œ")
                current_zone += 1

        # 's': ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ í›„ í•™ìŠµ/ì €ì¥
        if key == ord('s'):
            if all(v == SAMPLES_PER_ZONE for v in collected_counts.values()):
                train_and_save(np.array(features_data), np.array(labels_data), MODEL_PATH)
            else:
                print("âš  ì•„ì§ ëª¨ë“  êµ¬ì—­ ìˆ˜ì§‘ì´ ëë‚˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    print("[COLLECT] ì¢…ë£Œ")


def train_and_save(X: np.ndarray, y: np.ndarray, model_path: str):
    """
    RandomForestë¡œ í•™ìŠµ í›„ ëª¨ë¸ ì €ì¥.
    """
    print("\n--- Training Model ---")
    model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    model.fit(X, y)
    joblib.dump(model, model_path)
    print(f"Model saved â†’ {model_path}")


def predict_loop(cap: cv2.VideoCapture):
    """
    [PREDICT] ì‹¤ì‹œê°„ ì‹œì„  êµ¬ì—­ ì˜ˆì¸¡. í•™ìŠµëœ ëª¨ë¸(gaze_model.pkl)ì´ í•„ìš”í•˜ë‹¤.
    """
    # ëª¨ë¸ ë¡œë“œ
    try:
        model: RandomForestClassifier = joblib.load(MODEL_PATH)
        print("--- ì‹¤ì‹œê°„ ì˜ˆì¸¡ ëª¨ë“œ ---")
    except FileNotFoundError:
        print(f"ì˜¤ë¥˜: ëª¨ë¸ íŒŒì¼({MODEL_PATH})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € MODE='collect'ë¡œ í•™ìŠµí•˜ì„¸ìš”.")
        return

    while True:
        ok, frame = cap.read()
        if not ok:
            break

        vis_frame, features = process_frame_get_features(frame)
        display_frame = ensure_display_canvas()

        # ì˜ˆì¸¡
        if features is not None:
            pred_zone: int = int(model.predict([features])[0])
            zone_name = SCREEN_ZONES.get(pred_zone, "Unknown")
            text = f"Gaze Prediction: Zone {pred_zone} ({zone_name})"
            cv2.putText(display_frame, text, (50, 50),
                        cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 255), 3)

        cv2.imshow("Webcam Feed", vis_frame)
        cv2.imshow("Gaze Interface", display_frame)

        key = cv2.waitKey(1) & 0xFF
        if key == ord('q'):
            break

    print("[PREDICT] ì¢…ë£Œ")


# ==============================
# 4) ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
# ==============================
def main():
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("error: ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    try:
        if MODE == "collect":
            collect_loop(cap)
        elif MODE == "predict":
            predict_loop(cap)
        else:
            print(f"ì•Œ ìˆ˜ ì—†ëŠ” MODE: {MODE} (collect|predict ì¤‘ ì„ íƒ)")
    finally:
        cap.release()
        cv2.destroyAllWindows()


if __name__ == "__main__":
    main()
```
</details>

<details>
  <summary><h3>ëª¨ë¸</h3></summary>

```Python
import os
import cv2
import json
import time
import torch
import faiss
import requests
import numpy as np
from typing import Tuple, Optional
from pathlib import Path
from PIL import Image
from ultralytics import YOLO
from transformers import CLIPProcessor, CLIPModel

# -------------------------------------------------
# 0) ê¸°ë³¸ ì„¤ì •
# -------------------------------------------------
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

YOLO_WEIGHTS = "yolov8n.pt"
CLIP_ID = "openai/clip-vit-base-patch32"
FAISS_INDEX = "./data/faiss/met_text.index"
FAISS_META  = "./data/faiss/met_structured_with_objects.json"
BACKEND_URL = "http://3.34.240.201:8000"
REQUEST_TIMEOUT = 5

ART_CLASSES = {"tv", "book", "laptop", "cell phone", "remote", "keyboard", "monitor"}
MIN_CONF = 0.25             # YOLO ìµœì†Œ ì‹ ë¢°ë„
SELECTED_Q_DEFAULT = "Q1"   # /detect-area ê¸°ë³¸ Q
DEVICE = ("cuda" if torch.cuda.is_available()
          else ("mps" if torch.backends.mps.is_available() else "cpu"))

# -------------------------------------------------
# 1) ëª¨ë¸/ë¦¬ì†ŒìŠ¤ ë¡œë“œ
# -------------------------------------------------
def load_models() -> Tuple[YOLO, CLIPModel, CLIPProcessor, dict, faiss.Index, list]:
    """YOLO/CLIP/FAISS/ë©”íƒ€ ë¡œë“œ í›„ ë°˜í™˜"""
    yolo_model = YOLO(YOLO_WEIGHTS)

    clip_model = CLIPModel.from_pretrained(CLIP_ID).to(DEVICE)
    clip_processor = CLIPProcessor.from_pretrained(CLIP_ID)

    index = faiss.read_index(FAISS_INDEX)
    with open(FAISS_META, "r", encoding="utf-8") as f:
        image_meta = json.load(f)

    model_classes = yolo_model.model.names  # id->label ë§¤í•‘
    return yolo_model, clip_model, clip_processor, model_classes, index, image_meta

# -------------------------------------------------
# 2) ì„ë² ë”©/ê²€ìƒ‰/ì „ì†¡ ìœ í‹¸
# -------------------------------------------------
def embed_crop(image_bgr: np.ndarray, clip_model: CLIPModel, clip_processor: CLIPProcessor) -> np.ndarray:
    """
    crop(BGR) â†’ CLIP ì„ë² ë”©(float32, L2ì •ê·œí™”)
    """
    rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
    pil = Image.fromarray(rgb)
    inputs = clip_processor(images=pil, return_tensors="pt", padding=True).to(DEVICE)
    with torch.no_grad():
        emb = clip_model.get_image_features(**inputs)
        emb = emb / emb.norm(p=2, dim=-1, keepdim=True)
    return emb[0].detach().cpu().numpy().astype("float32")

def search_faiss(vec: np.ndarray, index: faiss.Index, meta: list) -> Tuple[str, float]:
    """
    CLIP ë²¡í„°â†’FAISS ê²€ìƒ‰â†’ (art_id, score) ë°˜í™˜
    ì£¼ì˜: index metricì— ë”°ë¼ score í•´ì„ ìƒì´(L2=ì‘ì„ìˆ˜ë¡ ìœ ì‚¬ / IP=í´ìˆ˜ë¡ ìœ ì‚¬)
    """
    D, I = index.search(vec.reshape(1, -1), k=1)
    idx = int(I[0][0])
    art_id = str(meta[idx]["full_image_id"])
    return art_id, float(D[0][0])

def post_backend_art(art_id: str) -> Optional[int]:
    """ /process-image?art_id=... POST """
    url = f"{BACKEND_URL}/process-image?art_id={art_id}"
    try:
        res = requests.post(url, timeout=REQUEST_TIMEOUT)
        print(f"[POST] {url} â†’ {res.status_code}")
        return res.status_code
    except requests.RequestException as e:
        print("[WARN] POST ì‹¤íŒ¨:", e)
        return None

def post_backend_area(art_id: str, q: str) -> Optional[int]:
    """ /process-image?art_id=...&q=Qx POST """
    url = f"{BACKEND_URL}/process-image?art_id={art_id}&q={q}"
    try:
        res = requests.post(url, timeout=REQUEST_TIMEOUT)
        print(f"[POST] {url} â†’ {res.status_code}")
        return res.status_code
    except requests.RequestException as e:
        print("[WARN] POST ì‹¤íŒ¨:", e)
        return None

# -------------------------------------------------
# 3) íƒì§€ ë¡œì§
# -------------------------------------------------
def detect_top_art(
    frame_bgr: np.ndarray,
    yolo_model: YOLO,
    model_classes: dict
) -> Optional[Tuple[str, float, Tuple[int, int, int, int]]]:
    """
    í”„ë ˆì„ì—ì„œ ê´€ì‹¬ ë¼ë²¨ë§Œ í•„í„°ë§ í›„, confê°€ ê°€ì¥ ë†’ì€ 1ê°œë¥¼ ë°˜í™˜.
    ë°˜í™˜: (label, conf, (x1,y1,x2,y2)) ë˜ëŠ” None
    """
    results = yolo_model(frame_bgr, verbose=False)[0]
    if results.boxes is None:
        return None

    candidates = []
    for box in results.boxes:
        cls_id = int(box.cls[0])
        label = model_classes[cls_id]
        conf = float(box.conf[0].item()) if box.conf is not None else 0.0
        if label in ART_CLASSES and conf >= MIN_CONF:
            x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())
            candidates.append((label, conf, (x1, y1, x2, y2)))

    if not candidates:
        return None

    # ìµœê³  ì‹ ë¢°ë„ 1ê°œ ì„ íƒ
    candidates.sort(key=lambda x: x[1], reverse=True)
    return candidates[0]

# -------------------------------------------------
# 4) ì‹œê°í™” í—¬í¼
# -------------------------------------------------
def draw_detection(frame: np.ndarray, label: str, art_id: str, score: float, box: Tuple[int, int, int, int]):
    """ë°•ìŠ¤ì™€ ë¼ë²¨/ID/ìŠ¤ì½”ì–´ ì˜¤ë²„ë ˆì´"""
    x1, y1, x2, y2 = box
    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 200, 0), 2)
    cv2.putText(frame, f"{label}: {art_id} ({score:.2f})",
                (x1, max(0, y1 - 8)),
                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (20, 80, 255), 2)

# -------------------------------------------------
# 5) ì‹¤í–‰ ë£¨í”„
# -------------------------------------------------
def run_loop(mode: str = "art", selected_q: str = SELECTED_Q_DEFAULT):
    """
    ì¹´ë©”ë¼ ì—´ê³  í”„ë ˆì„ ì²˜ë¦¬ â†’ ì²« ê°ì§€ 1ê±´ë§Œ ì „ì†¡ í›„ ì¢…ë£Œ.
    mode: 'art' | 'area'
    """
    # ë¦¬ì†ŒìŠ¤ ë¡œë“œ
    yolo_model, clip_model, clip_processor, model_classes, index, image_meta = load_models()

    # ëª¨ë“œ í™•ì¸
    mode = mode.lower().strip()
    if mode not in ("art", "area"):
        print("error: ì˜ëª»ëœ ëª¨ë“œì…ë‹ˆë‹¤. 'art' ë˜ëŠ” 'area' ì¤‘ ì„ íƒ.")
        return

    # ì¹´ë©”ë¼ ì‹œì‘
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("error: ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    latest_painting_id = None

    try:
        while True:
            ok, frame = cap.read()
            if not ok:
                break

            # YOLO íƒì§€ â†’ ìµœê³  conf 1ê°œ
            picked = detect_top_art(frame, yolo_model, model_classes)
            found = picked is not None

            if found:
                label, conf, (x1, y1, x2, y2) = picked
                crop = frame[y1:y2, x1:x2]
                if crop.size == 0:
                    found = False
                else:
                    # CLIP ì„ë² ë”© â†’ FAISS
                    vec = embed_crop(crop, clip_model, clip_processor)
                    art_id, score = search_faiss(vec, index, image_meta)
                    latest_painting_id = art_id

                    # ê·¸ë¦¬ê¸°
                    draw_detection(frame, label, art_id, score, (x1, y1, x2, y2))

            cv2.imshow("Art-Like Detection + FAISS", frame)

            # ê°ì§€ë˜ë©´ ë°±ì—”ë“œ ì „ì†¡í•˜ê³  ì¢…ë£Œ
            if found and latest_painting_id:
                print(f"ê°ì§€ëœ ê·¸ë¦¼ ID: {latest_painting_id}")
                if mode == "art":
                    post_backend_art(latest_painting_id)
                else:
                    post_backend_area(latest_painting_id, selected_q)
                break

            # ì¢…ë£Œ í‚¤
            if (cv2.waitKey(1) & 0xFF) == ord('q'):
                break

    finally:
        cap.release()
        cv2.destroyAllWindows()

    if latest_painting_id:
        print("ëª…ë ¹ ì‹¤í–‰ í›„ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    else:
        print("í”„ë¡œê·¸ë¨ì„ ì •ìƒ ì¢…ë£Œí–ˆìŠµë‹ˆë‹¤.")

# -------------------------------------------------
# 6) ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
# -------------------------------------------------
if __name__ == "__main__":
    # ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ì…ë ¥ìœ¼ë¡œ ëª¨ë“œ ì„ íƒ
    cmd = input("ë¨¼ì € ëª…ë ¹ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš” (/detect-art ë˜ëŠ” /detect-area): ").strip().lower()
    if cmd.startswith("/detect-art"):
        run_loop("art", SELECTED_Q_DEFAULT)
    elif cmd.startswith("/detect-area"):
        run_loop("area", SELECTED_Q_DEFAULT)  # í•„ìš” ì‹œ selected_që¥¼ ë™ì ìœ¼ë¡œ ë°”ê¿”ë„ ë¨
    else:
        print("error:ì˜ëª»ëœ ëª…ë ¹ì…ë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")

```
</details>
